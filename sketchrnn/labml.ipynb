{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Sketch RNN\n",
    "summary: >\n",
    "  This is an annotated PyTorch implementation of the Sketch RNN from paper A Neural Representation of Sketch Drawings.\n",
    "  Sketch RNN is a sequence-to-sequence model that generates sketches of objects such as bicycles, cats, etc.\n",
    "---\n",
    "\n",
    "# Sketch RNN\n",
    "\n",
    "This is an annotated [PyTorch](https://pytorch.org) implementation of the paper\n",
    "[A Neural Representation of Sketch Drawings](https://arxiv.org/abs/1704.03477).\n",
    "\n",
    "Sketch RNN is a sequence-to-sequence variational auto-encoder.\n",
    "Both encoder and decoder are recurrent neural network models.\n",
    "It learns to reconstruct stroke based simple drawings, by predicting\n",
    "a series of strokes.\n",
    "Decoder predicts each stroke as a mixture of Gaussian's.\n",
    "\n",
    "### Getting data\n",
    "Download data from [Quick, Draw! Dataset](https://github.com/googlecreativelab/quickdraw-dataset).\n",
    "There is a link to download `npz` files in *Sketch-RNN QuickDraw Dataset* section of the readme.\n",
    "Place the downloaded `npz` file(s) in `data/sketch` folder.\n",
    "This code is configured to use `bicycle` dataset.\n",
    "You can change this in configurations.\n",
    "\n",
    "### Acknowledgements\n",
    "Took help from [PyTorch Sketch RNN](https://github.com/alexis-jacq/Pytorch-Sketch-RNN) project by\n",
    "[Alexis David Jacq](https://github.com/alexis-jacq)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import einops\n",
    "from labml import lab, experiment, tracker, monit\n",
    "from labml_helpers.device import DeviceConfigs\n",
    "from labml_helpers.module import Module\n",
    "from labml_helpers.optimizer import OptimizerConfigs\n",
    "from labml_helpers.train_valid import TrainValidConfigs, hook_model_outputs, BatchIndex\n",
    "\n",
    "\n",
    "class StrokesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ## Dataset\n",
    "\n",
    "    This class loads and pre-processes the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: np.array, max_seq_length: int, scale: Optional[float] = None):\n",
    "        \"\"\"\n",
    "        `dataset` is a list of numpy arrays of shape [seq_len, 3].\n",
    "        It is a sequence of strokes, and each stroke is represented by\n",
    "        3 integers.\n",
    "        First two are the displacements along x and y ($\\Delta x$, $\\Delta y$)\n",
    "        and the last integer represents the state of the pen, $1$ if it's touching\n",
    "        the paper and $0$ otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        data = []\n",
    "        # We iterate through each of the sequences and filter\n",
    "        for seq in dataset:\n",
    "            # Filter if the length of the sequence of strokes is within our range\n",
    "            if 10 < len(seq) <= max_seq_length:\n",
    "                # Clamp $\\Delta x$, $\\Delta y$ to $[-1000, 1000]$\n",
    "                seq = np.minimum(seq, 1000)\n",
    "                seq = np.maximum(seq, -1000)\n",
    "                # Convert to a floating point array and add to `data`\n",
    "                seq = np.array(seq, dtype=np.float32)\n",
    "                data.append(seq)\n",
    "\n",
    "        # We then calculate the scaling factor which is the\n",
    "        # standard deviation of ($\\Delta x$, $\\Delta y$) combined.\n",
    "        # Paper notes that the mean is not adjusted for simplicity,\n",
    "        # since the mean is anyway close to $0$.\n",
    "        if scale is None:\n",
    "            scale = np.std(np.concatenate([np.ravel(s[:, 0:2]) for s in data]))\n",
    "        self.scale = scale\n",
    "\n",
    "        # Get the longest sequence length among all sequences\n",
    "        longest_seq_len = max([len(seq) for seq in data])\n",
    "\n",
    "        # We initialize PyTorch data array with two extra steps for start-of-sequence (sos)\n",
    "        # and end-of-sequence (eos).\n",
    "        # Each step is a vector $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
    "        # Only one of $p_1, p_2, p_3$ is $1$ and the others are $0$.\n",
    "        # They represent *pen down*, *pen up* and *end-of-sequence* in that order.\n",
    "        # $p_1$ is $1$ if the pen touches the paper in the next step.\n",
    "        # $p_2$ is $1$ if the pen doesn't touch the paper in the next step.\n",
    "        # $p_3$ is $1$ if it is the end of the drawing.\n",
    "        self.data = torch.zeros(len(data), longest_seq_len + 2, 5, dtype=torch.float)\n",
    "        # The mask array needs only one extra-step since it is for the outputs of the\n",
    "        # decoder, which takes in `data[:-1]` and predicts next step.\n",
    "        self.mask = torch.zeros(len(data), longest_seq_len + 1)\n",
    "\n",
    "        for i, seq in enumerate(data):\n",
    "            seq = torch.from_numpy(seq)\n",
    "            len_seq = len(seq)\n",
    "            # Scale and set $\\Delta x, \\Delta y$\n",
    "            self.data[i, 1:len_seq + 1, :2] = seq[:, :2] / scale\n",
    "            # $p_1$\n",
    "            self.data[i, 1:len_seq + 1, 2] = 1 - seq[:, 2]\n",
    "            # $p_2$\n",
    "            self.data[i, 1:len_seq + 1, 3] = seq[:, 2]\n",
    "            # $p_3$\n",
    "            self.data[i, len_seq + 1:, 4] = 1\n",
    "            # Mask is on until end of sequence\n",
    "            self.mask[i, :len_seq + 1] = 1\n",
    "\n",
    "        # Start-of-sequence is $(0, 0, 1, 0, 0)$\n",
    "        self.data[:, 0, 2] = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Size of the dataset\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Get a sample\"\"\"\n",
    "        return self.data[idx], self.mask[idx]\n",
    "\n",
    "\n",
    "class BivariateGaussianMixture:\n",
    "    \"\"\"\n",
    "    ## Bi-variate Gaussian mixture\n",
    "\n",
    "    The mixture is represented by $\\Pi$ and\n",
    "    $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
    "    This class adjusts temperatures and creates the categorical and Gaussian\n",
    "    distributions from the parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pi_logits: torch.Tensor, mu_x: torch.Tensor, mu_y: torch.Tensor,\n",
    "                 sigma_x: torch.Tensor, sigma_y: torch.Tensor, rho_xy: torch.Tensor):\n",
    "        self.pi_logits = pi_logits\n",
    "        self.mu_x = mu_x\n",
    "        self.mu_y = mu_y\n",
    "        self.sigma_x = sigma_x\n",
    "        self.sigma_y = sigma_y\n",
    "        self.rho_xy = rho_xy\n",
    "\n",
    "    @property\n",
    "    def n_distributions(self):\n",
    "        \"\"\"Number of distributions in the mixture, $M$\"\"\"\n",
    "        return self.pi_logits.shape[-1]\n",
    "\n",
    "    def set_temperature(self, temperature: float):\n",
    "        \"\"\"\n",
    "        Adjust by temperature $\\tau$\n",
    "        \"\"\"\n",
    "        # $$\\hat{\\Pi_k} \\leftarrow \\frac{\\hat{\\Pi_k}}{\\tau}$$\n",
    "        self.pi_logits /= temperature\n",
    "        # $$\\sigma^2_x \\leftarrow \\sigma^2_x \\tau$$\n",
    "        self.sigma_x *= math.sqrt(temperature)\n",
    "        # $$\\sigma^2_y \\leftarrow \\sigma^2_y \\tau$$\n",
    "        self.sigma_y *= math.sqrt(temperature)\n",
    "\n",
    "    def get_distribution(self):\n",
    "        # Clamp $\\sigma_x$, $\\sigma_y$ and $\\rho_{xy}$ to avoid getting `NaN`s\n",
    "        sigma_x = torch.clamp_min(self.sigma_x, 1e-5)\n",
    "        sigma_y = torch.clamp_min(self.sigma_y, 1e-5)\n",
    "        rho_xy = torch.clamp(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n",
    "\n",
    "        # Get means\n",
    "        mean = torch.stack([self.mu_x, self.mu_y], -1)\n",
    "        # Get covariance matrix\n",
    "        cov = torch.stack([\n",
    "            sigma_x * sigma_x, rho_xy * sigma_x * sigma_y,\n",
    "            rho_xy * sigma_x * sigma_y, sigma_y * sigma_y\n",
    "        ], -1)\n",
    "        cov = cov.view(*sigma_y.shape, 2, 2)\n",
    "\n",
    "        # Create bi-variate normal distribution.\n",
    "        #\n",
    "        # ðŸ“ It would be efficient to `scale_tril` matrix as `[[a, 0], [b, c]]`\n",
    "        # where\n",
    "        # $$a = \\sigma_x, b = \\rho_{xy} \\sigma_y, c = \\sigma_y \\sqrt{1 - \\rho^2_{xy}}$$.\n",
    "        # But for simplicity we use co-variance matrix.\n",
    "        # [This is a good resource](https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf)\n",
    "        # if you want to read up more about bi-variate distributions, their co-variance matrix,\n",
    "        # and probability density function.\n",
    "        multi_dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=cov)\n",
    "\n",
    "        # Create categorical distribution $\\Pi$ from logits\n",
    "        cat_dist = torch.distributions.Categorical(logits=self.pi_logits)\n",
    "\n",
    "        #\n",
    "        return cat_dist, multi_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderRNN(Module):\n",
    "    \"\"\"\n",
    "    ## Encoder module\n",
    "\n",
    "    This consists of a bidirectional LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_z: int, enc_hidden_size: int):\n",
    "        super().__init__()\n",
    "        # Create a bidirectional LSTM taking a sequence of\n",
    "        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$ as input.\n",
    "        # self.lstm = nn.LSTM(5, enc_hidden_size, bidirectional=True)\n",
    "        self.lstm = nn.LSTM(5, enc_hidden_size)\n",
    "        # self.lstm = nn.LSTM(5, enc_hidden_size)\n",
    "        # Head to get $\\mu$\n",
    "        # self.mu_head = nn.Linear(2 * enc_hidden_size, d_z)\n",
    "        self.mu_head = nn.Linear(enc_hidden_size, d_z)\n",
    "        # Head to get $\\hat{\\sigma}$\n",
    "        # self.sigma_head = nn.Linear(2 * enc_hidden_size, d_z)\n",
    "        self.sigma_head = nn.Linear(enc_hidden_size, d_z)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, state=None):\n",
    "        # The hidden state of the bidirectional LSTM is the concatenation of the\n",
    "        # output of the last token in the forward direction and\n",
    "        # first token in the reverse direction, which is what we want.\n",
    "        # $$h_{\\rightarrow} = encode_{\\rightarrow}(S),\n",
    "        # h_{\\leftarrow} = encodeâ†_{\\leftarrow}(S_{reverse}),\n",
    "        # h = [h_{\\rightarrow}; h_{\\leftarrow}]$$\n",
    "        \n",
    "        print(inputs.shape, \"inputs\")\n",
    "        _, (hidden, cell) = self.lstm(inputs.float(), state)\n",
    "        \n",
    "        print(hidden.shape, state, \"hidden\")\n",
    "        # The state has shape `[2, batch_size, hidden_size]`,\n",
    "        # where the first dimension is the direction.\n",
    "        # We rearrange it to get $h = [h_{\\rightarrow}; h_{\\leftarrow}]$\n",
    "        # hidden = einops.rearrange(hidden, 'fb b h -> b (fb h)')\n",
    "        hidden = hidden.squeeze(0)\n",
    "        print(hidden.shape, \"hidden_arranged\")\n",
    "\n",
    "        # $\\mu$\n",
    "        mu = self.mu_head(hidden)\n",
    "        # $\\hat{\\sigma}$\n",
    "        sigma_hat = self.sigma_head(hidden)\n",
    "        # $\\sigma = \\exp(\\frac{\\hat{\\sigma}}{2})$\n",
    "        sigma = torch.exp(sigma_hat / 2.)\n",
    "\n",
    "        # Sample $z = \\mu + \\sigma \\cdot \\mathcal{N}(0, I)$\n",
    "        print(mu.shape, sigma_hat.shape, \"z, mu, sigma_hat\")\n",
    "        z = mu + sigma * torch.normal(mu.new_zeros(mu.shape), mu.new_ones(mu.shape))\n",
    "\n",
    "        #\n",
    "        return z, mu, sigma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Module):\n",
    "    \"\"\"\n",
    "    ## Decoder module\n",
    "\n",
    "    This consists of a LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_z: int, dec_hidden_size: int, n_distributions: int):\n",
    "        super().__init__()\n",
    "        # LSTM takes $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ as input\n",
    "        self.lstm = nn.LSTM(d_z + 5, dec_hidden_size)\n",
    "\n",
    "        # Initial state of the LSTM is $[h_0; c_0] = \\tanh(W_{z}z + b_z)$.\n",
    "        # `init_state` is the linear transformation for this\n",
    "        self.init_state = nn.Linear(d_z, 2 * dec_hidden_size)\n",
    "\n",
    "        # This layer produces outputs for each of the `n_distributions`.\n",
    "        # Each distribution needs six parameters\n",
    "        # $(\\hat{\\Pi_i}, \\mu_{x_i}, \\mu_{y_i}, \\hat{\\sigma_{x_i}}, \\hat{\\sigma_{y_i}} \\hat{\\rho_{xy_i}})$\n",
    "        self.mixtures = nn.Linear(dec_hidden_size, 6 * n_distributions)\n",
    "\n",
    "        # This head is for the logits $(\\hat{q_1}, \\hat{q_2}, \\hat{q_3})$\n",
    "        self.q_head = nn.Linear(dec_hidden_size, 3)\n",
    "        # This is to calculate $\\log(q_k)$ where\n",
    "        # $$q_k = \\operatorname{softmax}(\\hat{q})_k = \\frac{\\exp(\\hat{q_k})}{\\sum_{j = 1}^3 \\exp(\\hat{q_j})}$$\n",
    "        self.q_log_softmax = nn.LogSoftmax(-1)\n",
    "\n",
    "        # These parameters are stored for future reference\n",
    "        self.n_distributions = n_distributions\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor, z: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        # Calculate the initial state\n",
    "        if state is None:\n",
    "            # $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
    "            print(x.shape)\n",
    "            print(\"hi\")\n",
    "            print(z.shape)\n",
    "            print(\"z\")\n",
    "            \n",
    "            print(self.init_state(z).shape)\n",
    "            h, c = torch.split(torch.tanh(self.init_state(z)), self.dec_hidden_size, 1)\n",
    "            # `h` and `c` have shapes `[batch_size, lstm_size]`. We want to shape them\n",
    "            # to `[1, batch_size, lstm_size]` because that's the shape used in LSTM.\n",
    "            state = (h.unsqueeze(0).contiguous(), c.unsqueeze(0).contiguous())\n",
    "\n",
    "        # Run the LSTM\n",
    "        outputs, state = self.lstm(x, state)\n",
    "\n",
    "        # Get $\\log(q)$\n",
    "        q_logits = self.q_log_softmax(self.q_head(outputs))\n",
    "\n",
    "        # Get $(\\hat{\\Pi_i}, \\mu_{x,i}, \\mu_{y,i}, \\hat{\\sigma_{x,i}},\n",
    "        # \\hat{\\sigma_{y,i}} \\hat{\\rho_{xy,i}})$.\n",
    "        # `torch.split` splits the output into 6 tensors of size `self.n_distribution`\n",
    "        # across dimension `2`.\n",
    "        pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy = \\\n",
    "            torch.split(self.mixtures(outputs), self.n_distributions, 2)\n",
    "\n",
    "        # Create a bi-variate Gaussian mixture\n",
    "        # $\\Pi$ and \n",
    "        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
    "        # where\n",
    "        # $$\\sigma_{x,i} = \\exp(\\hat{\\sigma_{x,i}}), \\sigma_{y,i} = \\exp(\\hat{\\sigma_{y,i}}),\n",
    "        # \\rho_{xy,i} = \\tanh(\\hat{\\rho_{xy,i}})$$\n",
    "        # and\n",
    "        # $$\\Pi_i = \\operatorname{softmax}(\\hat{\\Pi})_i = \\frac{\\exp(\\hat{\\Pi_i})}{\\sum_{j = 1}^3 \\exp(\\hat{\\Pi_j})}$$\n",
    "        #\n",
    "        # $\\Pi$ is the categorical probabilities of choosing the distribution out of the mixture\n",
    "        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
    "        dist = BivariateGaussianMixture(pi_logits, mu_x, mu_y,\n",
    "                                        torch.exp(sigma_x), torch.exp(sigma_y), torch.tanh(rho_xy))\n",
    "\n",
    "        #\n",
    "        return dist, q_logits, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionLoss(Module):\n",
    "    \"\"\"\n",
    "    ## Reconstruction Loss\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, mask: torch.Tensor, target: torch.Tensor,\n",
    "                 dist: 'BivariateGaussianMixture', q_logits: torch.Tensor):\n",
    "        # Get $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
    "        pi, mix = dist.get_distribution()\n",
    "        # `target` has shape `[seq_len, batch_size, 5]` where the last dimension is the features\n",
    "        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
    "        # We want to get $\\Delta x, \\Delta$ y and get the probabilities from each of the distributions\n",
    "        # in the mixture $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
    "        #\n",
    "        # `xy` will have shape `[seq_len, batch_size, n_distributions, 2]`\n",
    "        xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
    "        # Calculate the probabilities\n",
    "        # $$p(\\Delta x, \\Delta y) =\n",
    "        # \\sum_{j=1}^M \\Pi_j \\mathcal{N} \\big( \\Delta x, \\Delta y \\vert\n",
    "        # \\mu_{x,j}, \\mu_{y,j}, \\sigma_{x,j}, \\sigma_{y,j}, \\rho_{xy,j}\n",
    "        # \\big)$$\n",
    "        probs = torch.sum(pi.probs * torch.exp(mix.log_prob(xy)), 2)\n",
    "\n",
    "        # $$L_s = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_s} \\log \\big (p(\\Delta x, \\Delta y) \\big)$$\n",
    "        # Although `probs` has $N_{max}$ (`longest_seq_len`) elements, the sum is only taken\n",
    "        # upto $N_s$ because the rest is masked out.\n",
    "        #\n",
    "        # It might feel like we should be taking the sum and dividing by $N_s$ and not $N_{max}$,\n",
    "        # but this will give higher weight for individual predictions in shorter sequences.\n",
    "        # We give equal weight to each prediction $p(\\Delta x, \\Delta y)$ when we divide by $N_{max}$\n",
    "        loss_stroke = -torch.mean(mask * torch.log(1e-5 + probs))\n",
    "\n",
    "        # $$L_p = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_{max}} \\sum_{k=1}^{3} p_{k,i} \\log(q_{k,i})$$\n",
    "        loss_pen = -torch.mean(target[:, :, 2:] * q_logits)\n",
    "\n",
    "        # $$L_R = L_s + L_p$$\n",
    "        return loss_stroke + loss_pen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivLoss(Module):\n",
    "    \"\"\"\n",
    "    ## KL-Divergence loss\n",
    "\n",
    "    This calculates the KL divergence between a given normal distribution and $\\mathcal{N}(0, 1)$\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, sigma_hat: torch.Tensor, mu: torch.Tensor):\n",
    "        # $$L_{KL} = - \\frac{1}{2 N_z} \\bigg( 1 + \\hat{\\sigma} - \\mu^2 - \\exp(\\hat{\\sigma}) \\bigg)$$\n",
    "        return -0.5 * torch.mean(1 + sigma_hat - mu ** 2 - torch.exp(sigma_hat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "    \"\"\"\n",
    "    ## Sampler\n",
    "\n",
    "    This samples a sketch from the decoder and plots it\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN):\n",
    "        self.decoder = decoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def sample(self, data: torch.Tensor, temperature: float):\n",
    "        # $N_{max}$\n",
    "        longest_seq_len = len(data)\n",
    "\n",
    "        # Get $z$ from the encoder\n",
    "        z, _, _ = self.encoder(data)\n",
    "\n",
    "        # Start-of-sequence stroke is $(0, 0, 1, 0, 0)$\n",
    "        s = data.new_tensor([0, 0, 1, 0, 0])\n",
    "        seq = [s]\n",
    "        # Initial decoder is `None`.\n",
    "        # The decoder will initialize it to $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
    "        state = None\n",
    "\n",
    "        # We don't need gradients\n",
    "        with torch.no_grad():\n",
    "            # Sample $N_{max}$ strokes\n",
    "            for i in range(longest_seq_len):\n",
    "                # $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ is the input to the decoder\n",
    "                data = torch.cat([s.view(1, 1, -1), z.unsqueeze(0)], 2)\n",
    "                # Get $\\Pi$, $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$,\n",
    "                # $q$ and the next state from the decoder\n",
    "                dist, q_logits, state = self.decoder(data, z, state)\n",
    "                # Sample a stroke\n",
    "                s = self._sample_step(dist, q_logits, temperature)\n",
    "                # Add the new stroke to the sequence of strokes\n",
    "                seq.append(s)\n",
    "                # Stop sampling if $p_3 = 1$. This indicates that sketching has stopped\n",
    "                if s[4] == 1:\n",
    "                    break\n",
    "\n",
    "        # Create a PyTorch tensor of the sequence of strokes\n",
    "        seq = torch.stack(seq)\n",
    "\n",
    "        # Plot the sequence of strokes\n",
    "        self.plot(seq)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_step(dist: 'BivariateGaussianMixture', q_logits: torch.Tensor, temperature: float):\n",
    "        # Set temperature $\\tau$ for sampling. This is implemented in class `BivariateGaussianMixture`.\n",
    "        dist.set_temperature(temperature)\n",
    "        # Get temperature adjusted $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
    "        pi, mix = dist.get_distribution()\n",
    "        # Sample from $\\Pi$ the index of the distribution to use from the mixture\n",
    "        idx = pi.sample()[0, 0]\n",
    "\n",
    "        # Create categorical distribution $q$ with log-probabilities `q_logits` or $\\hat{q}$\n",
    "        q = torch.distributions.Categorical(logits=q_logits / temperature)\n",
    "        # Sample from $q$\n",
    "        q_idx = q.sample()[0, 0]\n",
    "\n",
    "        # Sample from the normal distributions in the mixture and pick the one indexed by `idx`\n",
    "        xy = mix.sample()[0, 0, idx]\n",
    "\n",
    "        # Create an empty stroke $(\\Delta x, \\Delta y, q_1, q_2, q_3)$\n",
    "        stroke = q_logits.new_zeros(5)\n",
    "        # Set $\\Delta x, \\Delta y$\n",
    "        stroke[:2] = xy\n",
    "        # Set $q_1, q_2, q_3$\n",
    "        stroke[q_idx + 2] = 1\n",
    "        #\n",
    "        return stroke\n",
    "\n",
    "    @staticmethod\n",
    "    def plot(seq: torch.Tensor):\n",
    "        # Take the cumulative sums of $(\\Delta x, \\Delta y)$ to get $(x, y)$\n",
    "        seq[:, 0:2] = torch.cumsum(seq[:, 0:2], dim=0)\n",
    "        # Create a new numpy array of the form $(x, y, q_2)$\n",
    "        seq[:, 2] = seq[:, 3]\n",
    "        seq = seq[:, 0:3].detach().cpu().numpy()\n",
    "\n",
    "        # Split the array at points where $q_2$ is $1$.\n",
    "        # i.e. split the array of strokes at the points where the pen is lifted from the paper.\n",
    "        # This gives a list of sequence of strokes.\n",
    "        strokes = np.split(seq, np.where(seq[:, 2] > 0)[0] + 1)\n",
    "        # Plot each sequence of strokes\n",
    "        for s in strokes:\n",
    "            plt.plot(s[:, 0], -s[:, 1])\n",
    "        # Don't show axes\n",
    "        plt.axis('off')\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>Not a valid git repository: <strong>/Users/hunkim/Github/pixel-art-ai/sketchrnn</strong><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([118, 1, 5]) inputs\n",
      "torch.Size([1, 1, 256]) hidden\n",
      "torch.Size([1, 256]) hidden_arranged\n",
      "torch.Size([1, 128]) torch.Size([1, 128]) z, mu, sigma_hat\n",
      "torch.Size([1, 1, 133])\n",
      "hi\n",
      "torch.Size([1, 128])\n",
      "z\n",
      "torch.Size([1, 1024])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n",
      "torch.Size([118, 4, 5]) inputs\n",
      "torch.Size([1, 4, 256]) hidden\n",
      "torch.Size([4, 256]) hidden_arranged\n",
      "torch.Size([4, 128]) torch.Size([4, 128]) z, mu, sigma_hat\n",
      "torch.Size([117, 4, 133])\n",
      "hi\n",
      "torch.Size([4, 128])\n",
      "z\n",
      "torch.Size([4, 1024])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 177\u001b[0m\n\u001b[1;32m    173\u001b[0m         configs\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[59], line 173\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m experiment\u001b[38;5;241m.\u001b[39mconfigs(configs, {\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer.optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# We use a learning rate of `1e-3` because we can see results faster.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    169\u001b[0m })\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mstart():\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/train_valid.py:269\u001b[0m, in \u001b[0;36mTrainValidConfigs.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_loop:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/train_valid.py:257\u001b[0m, in \u001b[0;36mTrainValidConfigs.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mupdate(is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39mnamespace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 257\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39mnamespace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/train_valid.py:140\u001b[0m, in \u001b[0;36mTrainer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m         sm\u001b[38;5;241m.\u001b[39mon_epoch_start()\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mis_train):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__iterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_index\u001b[38;5;241m.\u001b[39mcompleted:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_modules:\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/train_valid.py:153\u001b[0m, in \u001b[0;36mTrainer.__iterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_index\u001b[38;5;241m.\u001b[39miteration_completed:\n\u001b[1;32m    151\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iterable)\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_index\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    156\u001b[0m     monit\u001b[38;5;241m.\u001b[39mprogress(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_index\u001b[38;5;241m.\u001b[39mepoch_progress)\n",
      "Cell \u001b[0;32mIn[59], line 111\u001b[0m, in \u001b[0;36mConfigs.step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([data[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], z_stack], \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Get mixture of distributions and $\\hat{q}$\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     dist, q_logits, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m monit\u001b[38;5;241m.\u001b[39msection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# $L_{KL}$\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[55], line 48\u001b[0m, in \u001b[0;36mDecoderRNN.forward\u001b[0;34m(self, x, z, state)\u001b[0m\n\u001b[1;32m     45\u001b[0m     state \u001b[38;5;241m=\u001b[39m (h\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous(), c\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous())\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Run the LSTM\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m outputs, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Get $\\log(q)$\u001b[39;00m\n\u001b[1;32m     51\u001b[0m q_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_log_softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_head(outputs))\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/training_loop.py:175\u001b[0m, in \u001b[0;36mTrainingLoop.__handler\u001b[0;34m(self, sig, frame)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__finish()\n\u001b[1;32m    174\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKilling loop...\u001b[39m\u001b[38;5;124m'\u001b[39m, Text\u001b[38;5;241m.\u001b[39mdanger)\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mold_handler(sig, frame)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Configs(TrainValidConfigs):\n",
    "    \"\"\"\n",
    "    ## Configurations\n",
    "\n",
    "    These are default configurations which can later be adjusted by passing a `dict`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Device configurations to pick the device to run the experiment\n",
    "    device: torch.device = DeviceConfigs()\n",
    "    #\n",
    "    encoder: EncoderRNN\n",
    "    decoder: DecoderRNN\n",
    "    optimizer: optim.Adam\n",
    "    sampler: Sampler\n",
    "\n",
    "    dataset_name: str\n",
    "    train_loader: DataLoader\n",
    "    valid_loader: DataLoader\n",
    "    train_dataset: StrokesDataset\n",
    "    valid_dataset: StrokesDataset\n",
    "\n",
    "    # Encoder and decoder sizes\n",
    "    enc_hidden_size = 256\n",
    "    dec_hidden_size = 512\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = 4\n",
    "\n",
    "    # Number of features in $z$\n",
    "    d_z = 128\n",
    "    # Number of distributions in the mixture, $M$\n",
    "    n_distributions = 20\n",
    "\n",
    "    # Weight of KL divergence loss, $w_{KL}$\n",
    "    kl_div_loss_weight = 0.5\n",
    "    # Gradient clipping\n",
    "    grad_clip = 1.\n",
    "    # Temperature $\\tau$ for sampling\n",
    "    temperature = 0.4\n",
    "\n",
    "    # Filter out stroke sequences longer than $200$\n",
    "    max_seq_length = 200\n",
    "\n",
    "    epochs = 1 \n",
    "\n",
    "    kl_div_loss = KLDivLoss()\n",
    "    reconstruction_loss = ReconstructionLoss()\n",
    "\n",
    "    def init(self):\n",
    "        # Initialize encoder & decoder\n",
    "        self.encoder = EncoderRNN(self.d_z, self.enc_hidden_size).to(self.device)\n",
    "        self.decoder = DecoderRNN(self.d_z, self.dec_hidden_size, self.n_distributions).to(self.device)\n",
    "\n",
    "        # Set optimizer. Things like type of optimizer and learning rate are configurable\n",
    "        optimizer = OptimizerConfigs()\n",
    "        optimizer.parameters = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Create sampler\n",
    "        self.sampler = Sampler(self.encoder, self.decoder)\n",
    "\n",
    "        # `npz` file path is `data/sketch/[DATASET NAME].npz`\n",
    "        path = lab.get_data_path() / 'sketch' / f'{self.dataset_name}.npz'\n",
    "        # Load the numpy file\n",
    "        dataset = np.load(str(path), encoding='latin1', allow_pickle=True)\n",
    "\n",
    "        # Create training dataset\n",
    "        self.train_dataset = StrokesDataset(dataset['train'], self.max_seq_length)\n",
    "        # Create validation dataset\n",
    "        self.valid_dataset = StrokesDataset(dataset['valid'], self.max_seq_length, self.train_dataset.scale)\n",
    "\n",
    "        # Create training data loader\n",
    "        self.train_loader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
    "        # Create validation data loader\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, self.batch_size)\n",
    "\n",
    "        # Add hooks to monitor layer outputs on Tensorboard\n",
    "        hook_model_outputs(self.mode, self.encoder, 'encoder')\n",
    "        hook_model_outputs(self.mode, self.decoder, 'decoder')\n",
    "\n",
    "        # Configure the tracker to print the total train/validation loss\n",
    "        tracker.set_scalar(\"loss.total.*\", True)\n",
    "\n",
    "        self.state_modules = []\n",
    "\n",
    "    def step(self, batch: Any, batch_idx: BatchIndex):\n",
    "        self.encoder.train(self.mode.is_train)\n",
    "        self.decoder.train(self.mode.is_train)\n",
    "\n",
    "        # Move `data` and `mask` to device and swap the sequence and batch dimensions.\n",
    "        # `data` will have shape `[seq_len, batch_size, 5]` and\n",
    "        # `mask` will have shape `[seq_len, batch_size]`.\n",
    "        data = batch[0].to(self.device).transpose(0, 1)\n",
    "        mask = batch[1].to(self.device).transpose(0, 1)\n",
    "\n",
    "        # Increment step in training mode\n",
    "        if self.mode.is_train:\n",
    "            tracker.add_global_step(len(data))\n",
    "\n",
    "        # Encode the sequence of strokes\n",
    "        with monit.section(\"encoder\"):\n",
    "            # Get $z$, $\\mu$, and $\\hat{\\sigma}$\n",
    "            z, mu, sigma_hat = self.encoder(data)\n",
    "\n",
    "        # Decode the mixture of distributions and $\\hat{q}$\n",
    "        with monit.section(\"decoder\"):\n",
    "            # Concatenate $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$\n",
    "            z_stack = z.unsqueeze(0).expand(data.shape[0] - 1, -1, -1)\n",
    "            inputs = torch.cat([data[:-1], z_stack], 2)\n",
    "            # Get mixture of distributions and $\\hat{q}$\n",
    "            dist, q_logits, _ = self.decoder(inputs, z, None)\n",
    "\n",
    "        # Compute the loss\n",
    "        with monit.section('loss'):\n",
    "            # $L_{KL}$\n",
    "            kl_loss = self.kl_div_loss(sigma_hat, mu)\n",
    "            # $L_R$\n",
    "            reconstruction_loss = self.reconstruction_loss(mask, data[1:], dist, q_logits)\n",
    "            # $Loss = L_R + w_{KL} L_{KL}$\n",
    "            loss = reconstruction_loss + self.kl_div_loss_weight * kl_loss\n",
    "\n",
    "            # Track losses\n",
    "            tracker.add(\"loss.kl.\", kl_loss)\n",
    "            tracker.add(\"loss.reconstruction.\", reconstruction_loss)\n",
    "            tracker.add(\"loss.total.\", loss)\n",
    "\n",
    "        # Only if we are in training state\n",
    "        if self.mode.is_train:\n",
    "            # Run optimizer\n",
    "            with monit.section('optimize'):\n",
    "                # Set `grad` to zero\n",
    "                self.optimizer.zero_grad()\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                # Log model parameters and gradients\n",
    "                if batch_idx.is_last:\n",
    "                    tracker.add(encoder=self.encoder, decoder=self.decoder)\n",
    "                # Clip gradients\n",
    "                nn.utils.clip_grad_norm_(self.encoder.parameters(), self.grad_clip)\n",
    "                nn.utils.clip_grad_norm_(self.decoder.parameters(), self.grad_clip)\n",
    "                # Optimize\n",
    "                self.optimizer.step()\n",
    "\n",
    "        tracker.save()\n",
    "\n",
    "    def sample(self):\n",
    "        # Randomly pick a sample from validation dataset to encoder\n",
    "        data, *_ = self.valid_dataset[np.random.choice(len(self.valid_dataset))]\n",
    "        # Add batch dimension and move it to device\n",
    "        data = data.unsqueeze(1).to(self.device)\n",
    "        # Sample\n",
    "        self.sampler.sample(data, self.temperature)\n",
    "\n",
    "\n",
    "def main():\n",
    "    configs = Configs()\n",
    "    experiment.create(name=\"sketch_rnn\")\n",
    "\n",
    "    # Pass a dictionary of configurations\n",
    "    experiment.configs(configs, {\n",
    "        'optimizer.optimizer': 'Adam',\n",
    "        # We use a learning rate of `1e-3` because we can see results faster.\n",
    "        # Paper had suggested `1e-4`.\n",
    "        'optimizer.learning_rate': 1e-3,\n",
    "        # Name of the dataset\n",
    "        'dataset_name': 'bicycle',\n",
    "        # Number of inner iterations within an epoch to switch between training, validation and sampling.\n",
    "        'inner_iterations': 10\n",
    "    })\n",
    "\n",
    "    with experiment.start():\n",
    "        # Run the experiment\n",
    "        configs.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
