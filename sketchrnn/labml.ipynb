{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "---\n",
    "title: Sketch RNN\n",
    "summary: >\n",
    "  This is an annotated PyTorch implementation of the Sketch RNN from paper A Neural Representation of Sketch Drawings.\n",
    "  Sketch RNN is a sequence-to-sequence model that generates sketches of objects such as bicycles, cats, etc.\n",
    "---\n",
    "\n",
    "# Sketch RNN\n",
    "\n",
    "This is an annotated [PyTorch](https://pytorch.org) implementation of the paper\n",
    "[A Neural Representation of Sketch Drawings](https://arxiv.org/abs/1704.03477).\n",
    "\n",
    "Sketch RNN is a sequence-to-sequence variational auto-encoder.\n",
    "Both encoder and decoder are recurrent neural network models.\n",
    "It learns to reconstruct stroke based simple drawings, by predicting\n",
    "a series of strokes.\n",
    "Decoder predicts each stroke as a mixture of Gaussian's.\n",
    "\n",
    "### Getting data\n",
    "Download data from [Quick, Draw! Dataset](https://github.com/googlecreativelab/quickdraw-dataset).\n",
    "There is a link to download `npz` files in *Sketch-RNN QuickDraw Dataset* section of the readme.\n",
    "Place the downloaded `npz` file(s) in `data/sketch` folder.\n",
    "This code is configured to use `bicycle` dataset.\n",
    "You can change this in configurations.\n",
    "\n",
    "### Acknowledgements\n",
    "Took help from [PyTorch Sketch RNN](https://github.com/alexis-jacq/Pytorch-Sketch-RNN) project by\n",
    "[Alexis David Jacq](https://github.com/alexis-jacq)\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "from typing import Optional, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import einops\n",
    "from labml import lab, experiment, tracker, monit\n",
    "from labml_helpers.device import DeviceConfigs\n",
    "from labml_helpers.module import Module\n",
    "from labml_helpers.optimizer import OptimizerConfigs\n",
    "from labml_helpers.train_valid import TrainValidConfigs, hook_model_outputs, BatchIndex\n",
    "\n",
    "\n",
    "class StrokesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    ## Dataset\n",
    "\n",
    "    This class loads and pre-processes the data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: np.array, max_seq_length: int, scale: Optional[float] = None):\n",
    "        \"\"\"\n",
    "        `dataset` is a list of numpy arrays of shape [seq_len, 3].\n",
    "        It is a sequence of strokes, and each stroke is represented by\n",
    "        3 integers.\n",
    "        First two are the displacements along x and y ($\\Delta x$, $\\Delta y$)\n",
    "        and the last integer represents the state of the pen, $1$ if it's touching\n",
    "        the paper and $0$ otherwise.\n",
    "        \"\"\"\n",
    "\n",
    "        data = []\n",
    "        # We iterate through each of the sequences and filter\n",
    "        for seq in dataset:\n",
    "            # Filter if the length of the sequence of strokes is within our range\n",
    "            if 10 < len(seq) <= max_seq_length:\n",
    "                # Clamp $\\Delta x$, $\\Delta y$ to $[-1000, 1000]$\n",
    "                seq = np.minimum(seq, 1000)\n",
    "                seq = np.maximum(seq, -1000)\n",
    "                # Convert to a floating point array and add to `data`\n",
    "                seq = np.array(seq, dtype=np.float32)\n",
    "                data.append(seq)\n",
    "\n",
    "        # We then calculate the scaling factor which is the\n",
    "        # standard deviation of ($\\Delta x$, $\\Delta y$) combined.\n",
    "        # Paper notes that the mean is not adjusted for simplicity,\n",
    "        # since the mean is anyway close to $0$.\n",
    "        if scale is None:\n",
    "            scale = np.std(np.concatenate([np.ravel(s[:, 0:2]) for s in data]))\n",
    "        self.scale = scale\n",
    "\n",
    "        # Get the longest sequence length among all sequences\n",
    "        longest_seq_len = max([len(seq) for seq in data])\n",
    "\n",
    "        # We initialize PyTorch data array with two extra steps for start-of-sequence (sos)\n",
    "        # and end-of-sequence (eos).\n",
    "        # Each step is a vector $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
    "        # Only one of $p_1, p_2, p_3$ is $1$ and the others are $0$.\n",
    "        # They represent *pen down*, *pen up* and *end-of-sequence* in that order.\n",
    "        # $p_1$ is $1$ if the pen touches the paper in the next step.\n",
    "        # $p_2$ is $1$ if the pen doesn't touch the paper in the next step.\n",
    "        # $p_3$ is $1$ if it is the end of the drawing.\n",
    "        self.data = torch.zeros(len(data), longest_seq_len + 2, 5, dtype=torch.float)\n",
    "        # The mask array needs only one extra-step since it is for the outputs of the\n",
    "        # decoder, which takes in `data[:-1]` and predicts next step.\n",
    "        self.mask = torch.zeros(len(data), longest_seq_len + 1)\n",
    "        \n",
    "        # print(self.mask.shape, self.data.shape, 'dataset!')\n",
    "\n",
    "        for i, seq in enumerate(data):\n",
    "            seq = torch.from_numpy(seq)\n",
    "            len_seq = len(seq)\n",
    "            # Scale and set $\\Delta x, \\Delta y$\n",
    "            self.data[i, 1:len_seq + 1, :2] = seq[:, :2] / scale\n",
    "            # $p_1$\n",
    "            self.data[i, 1:len_seq + 1, 2] = 1 - seq[:, 2]\n",
    "            # $p_2$\n",
    "            self.data[i, 1:len_seq + 1, 3] = seq[:, 2]\n",
    "            # $p_3$\n",
    "            self.data[i, len_seq + 1:, 4] = 1\n",
    "            # Mask is on until end of sequence\n",
    "            self.mask[i, :len_seq + 1] = 1\n",
    "\n",
    "        # Start-of-sequence is $(0, 0, 1, 0, 0)$\n",
    "        self.data[:, 0, 2] = 1\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Size of the dataset\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"Get a sample\"\"\"\n",
    "        return self.data[idx], self.mask[idx]\n",
    "\n",
    "\n",
    "class BivariateGaussianMixture:\n",
    "    \"\"\"\n",
    "    ## Bi-variate Gaussian mixture\n",
    "\n",
    "    The mixture is represented by $\\Pi$ and\n",
    "    $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
    "    This class adjusts temperatures and creates the categorical and Gaussian\n",
    "    distributions from the parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pi_logits: torch.Tensor, mu_x: torch.Tensor, mu_y: torch.Tensor,\n",
    "                 sigma_x: torch.Tensor, sigma_y: torch.Tensor, rho_xy: torch.Tensor):\n",
    "        self.pi_logits = pi_logits\n",
    "        self.mu_x = mu_x\n",
    "        self.mu_y = mu_y\n",
    "        self.sigma_x = sigma_x\n",
    "        self.sigma_y = sigma_y\n",
    "        self.rho_xy = rho_xy\n",
    "\n",
    "    @property\n",
    "    def n_distributions(self):\n",
    "        \"\"\"Number of distributions in the mixture, $M$\"\"\"\n",
    "        return self.pi_logits.shape[-1]\n",
    "\n",
    "    def set_temperature(self, temperature: float):\n",
    "        \"\"\"\n",
    "        Adjust by temperature $\\tau$\n",
    "        \"\"\"\n",
    "        # $$\\hat{\\Pi_k} \\leftarrow \\frac{\\hat{\\Pi_k}}{\\tau}$$\n",
    "        self.pi_logits /= temperature\n",
    "        # $$\\sigma^2_x \\leftarrow \\sigma^2_x \\tau$$\n",
    "        self.sigma_x *= math.sqrt(temperature)\n",
    "        # $$\\sigma^2_y \\leftarrow \\sigma^2_y \\tau$$\n",
    "        self.sigma_y *= math.sqrt(temperature)\n",
    "\n",
    "    def get_distribution(self):\n",
    "        # Clamp $\\sigma_x$, $\\sigma_y$ and $\\rho_{xy}$ to avoid getting `NaN`s\n",
    "        sigma_x = torch.clamp_min(self.sigma_x, 1e-5)\n",
    "        sigma_y = torch.clamp_min(self.sigma_y, 1e-5)\n",
    "        rho_xy = torch.clamp(self.rho_xy, -1 + 1e-5, 1 - 1e-5)\n",
    "\n",
    "        # Get means\n",
    "        mean = torch.stack([self.mu_x, self.mu_y], -1)\n",
    "        # Get covariance matrix\n",
    "        cov = torch.stack([\n",
    "            sigma_x * sigma_x, rho_xy * sigma_x * sigma_y,\n",
    "            rho_xy * sigma_x * sigma_y, sigma_y * sigma_y\n",
    "        ], -1)\n",
    "        cov = cov.view(*sigma_y.shape, 2, 2)\n",
    "\n",
    "        # Create bi-variate normal distribution.\n",
    "        #\n",
    "        # ðŸ“ It would be efficient to `scale_tril` matrix as `[[a, 0], [b, c]]`\n",
    "        # where\n",
    "        # $$a = \\sigma_x, b = \\rho_{xy} \\sigma_y, c = \\sigma_y \\sqrt{1 - \\rho^2_{xy}}$$.\n",
    "        # But for simplicity we use co-variance matrix.\n",
    "        # [This is a good resource](https://www2.stat.duke.edu/courses/Spring12/sta104.1/Lectures/Lec22.pdf)\n",
    "        # if you want to read up more about bi-variate distributions, their co-variance matrix,\n",
    "        # and probability density function.\n",
    "        multi_dist = torch.distributions.MultivariateNormal(mean, covariance_matrix=cov)\n",
    "\n",
    "        # Create categorical distribution $\\Pi$ from logits\n",
    "        cat_dist = torch.distributions.Categorical(logits=self.pi_logits)\n",
    "\n",
    "        #\n",
    "        return cat_dist, multi_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderRNN(Module):\n",
    "    \"\"\"\n",
    "    ## Encoder module\n",
    "\n",
    "    This consists of a bidirectional LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_z: int, enc_hidden_size: int):\n",
    "        super().__init__()\n",
    "        # Create a bidirectional LSTM taking a sequence of\n",
    "        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$ as input.\n",
    "        # self.lstm = nn.LSTM(5, enc_hidden_size, bidirectional=True)\n",
    "        self.lstm = nn.LSTM(5, enc_hidden_size)\n",
    "        # self.lstm = nn.LSTM(5, enc_hidden_size)\n",
    "        # Head to get $\\mu$\n",
    "        # self.mu_head = nn.Linear(2 * enc_hidden_size, d_z)\n",
    "        self.mu_head = nn.Linear(enc_hidden_size, d_z)\n",
    "        # Head to get $\\hat{\\sigma}$\n",
    "        # self.sigma_head = nn.Linear(2 * enc_hidden_size, d_z)\n",
    "        self.sigma_head = nn.Linear(enc_hidden_size, d_z)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor, state=None):\n",
    "        # The hidden state of the bidirectional LSTM is the concatenation of the\n",
    "        # output of the last token in the forward direction and\n",
    "        # first token in the reverse direction, which is what we want.\n",
    "        # $$h_{\\rightarrow} = encode_{\\rightarrow}(S),\n",
    "        # h_{\\leftarrow} = encodeâ†_{\\leftarrow}(S_{reverse}),\n",
    "        # h = [h_{\\rightarrow}; h_{\\leftarrow}]$$\n",
    "        \n",
    "        _, (hidden, cell) = self.lstm(inputs.float(), state)\n",
    "        \n",
    "        # The state has shape `[2, batch_size, hidden_size]`,\n",
    "        # where the first dimension is the direction.\n",
    "        # We rearrange it to get $h = [h_{\\rightarrow}; h_{\\leftarrow}]$\n",
    "        # hidden = einops.rearrange(hidden, 'fb b h -> b (fb h)')\n",
    "        hidden = hidden.squeeze(0)\n",
    "\n",
    "        # $\\mu$\n",
    "        mu = self.mu_head(hidden)\n",
    "        # $\\hat{\\sigma}$\n",
    "        sigma_hat = self.sigma_head(hidden)\n",
    "        # $\\sigma = \\exp(\\frac{\\hat{\\sigma}}{2})$\n",
    "        sigma = torch.exp(sigma_hat / 2.)\n",
    "\n",
    "        # Sample $z = \\mu + \\sigma \\cdot \\mathcal{N}(0, I)$\n",
    "        z = mu + sigma * torch.normal(mu.new_zeros(mu.shape), mu.new_ones(mu.shape))\n",
    "\n",
    "        #\n",
    "        return z, mu, sigma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(Module):\n",
    "    \"\"\"\n",
    "    ## Decoder module\n",
    "\n",
    "    This consists of a LSTM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_z: int, dec_hidden_size: int, n_distributions: int):\n",
    "        super().__init__()\n",
    "        # LSTM takes $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ as input\n",
    "        self.lstm = nn.LSTM(d_z + 5, dec_hidden_size)\n",
    "\n",
    "        # Initial state of the LSTM is $[h_0; c_0] = \\tanh(W_{z}z + b_z)$.\n",
    "        # `init_state` is the linear transformation for this\n",
    "        self.init_state = nn.Linear(d_z, 2 * dec_hidden_size)\n",
    "\n",
    "        # This layer produces outputs for each of the `n_distributions`.\n",
    "        # Each distribution needs six parameters\n",
    "        # $(\\hat{\\Pi_i}, \\mu_{x_i}, \\mu_{y_i}, \\hat{\\sigma_{x_i}}, \\hat{\\sigma_{y_i}} \\hat{\\rho_{xy_i}})$\n",
    "        self.mixtures = nn.Linear(dec_hidden_size, 6 * n_distributions)\n",
    "\n",
    "        # This head is for the logits $(\\hat{q_1}, \\hat{q_2}, \\hat{q_3})$\n",
    "        self.q_head = nn.Linear(dec_hidden_size, 3)\n",
    "        # This is to calculate $\\log(q_k)$ where\n",
    "        # $$q_k = \\operatorname{softmax}(\\hat{q})_k = \\frac{\\exp(\\hat{q_k})}{\\sum_{j = 1}^3 \\exp(\\hat{q_j})}$$\n",
    "        self.q_log_softmax = nn.LogSoftmax(-1)\n",
    "\n",
    "        # These parameters are stored for future reference\n",
    "        self.n_distributions = n_distributions\n",
    "        self.dec_hidden_size = dec_hidden_size\n",
    "\n",
    "    def forward(self, x: torch.Tensor, z: torch.Tensor, state: Optional[Tuple[torch.Tensor, torch.Tensor]]):\n",
    "        # Calculate the initial state\n",
    "        if state is None:\n",
    "            # $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
    "        \n",
    "            h, c = torch.split(torch.tanh(self.init_state(z)), self.dec_hidden_size, 1)\n",
    "            # `h` and `c` have shapes `[batch_size, lstm_size]`. We want to shape them\n",
    "            # to `[1, batch_size, lstm_size]` because that's the shape used in LSTM.\n",
    "            state = (h.unsqueeze(0).contiguous(), c.unsqueeze(0).contiguous())\n",
    "            \n",
    "            # state is the state passed from the previous step (or encoder)\n",
    "\n",
    "        # Run the LSTM\n",
    "        outputs, state = self.lstm(x, state)\n",
    "\n",
    "        # Get $\\log(q)$  \n",
    "        q_logits = self.q_log_softmax(self.q_head(outputs))\n",
    "        \n",
    "\n",
    "        # Get $(\\hat{\\Pi_i}, \\mu_{x,i}, \\mu_{y,i}, \\hat{\\sigma_{x,i}},\n",
    "        # \\hat{\\sigma_{y,i}} \\hat{\\rho_{xy,i}})$.\n",
    "        # `torch.split` splits the output into 6 tensors of size `self.n_distribution`\n",
    "        # across dimension `2`.\n",
    "        # outputs shape is [seq_len, batch_size, lstm_size]\n",
    "        pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy = \\\n",
    "            torch.split(self.mixtures(outputs), self.n_distributions, 2)\n",
    "            \n",
    "        # Create a bi-variate Gaussian mixture\n",
    "        # $\\Pi$ and \n",
    "        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
    "        # where\n",
    "        # $$\\sigma_{x,i} = \\exp(\\hat{\\sigma_{x,i}}), \\sigma_{y,i} = \\exp(\\hat{\\sigma_{y,i}}),\n",
    "        # \\rho_{xy,i} = \\tanh(\\hat{\\rho_{xy,i}})$$\n",
    "        # and\n",
    "        # $$\\Pi_i = \\operatorname{softmax}(\\hat{\\Pi})_i = \\frac{\\exp(\\hat{\\Pi_i})}{\\sum_{j = 1}^3 \\exp(\\hat{\\Pi_j})}$$\n",
    "        #\n",
    "        # $\\Pi$ is the categorical probabilities of choosing the distribution out of the mixture\n",
    "        # $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
    "        dist = BivariateGaussianMixture(pi_logits, mu_x, mu_y,\n",
    "                                        torch.exp(sigma_x), torch.exp(sigma_y), torch.tanh(rho_xy))\n",
    "\n",
    "        #\n",
    "        return dist, q_logits, state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconstructionLoss(Module):\n",
    "    \"\"\"\n",
    "    ## Reconstruction Loss\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, mask: torch.Tensor, target: torch.Tensor,\n",
    "                 dist: 'BivariateGaussianMixture', q_logits: torch.Tensor):\n",
    "        # Get $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
    "        pi, mix = dist.get_distribution()\n",
    "        # `target` has shape `[seq_len, batch_size, 5]` where the last dimension is the features\n",
    "        # $(\\Delta x, \\Delta y, p_1, p_2, p_3)$.\n",
    "        # We want to get $\\Delta x, \\Delta$ y and get the probabilities from each of the distributions\n",
    "        # in the mixture $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$.\n",
    "        #\n",
    "        # `xy` will have shape `[seq_len, batch_size, n_distributions, 2]`\n",
    "        xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
    "        # Calculate the probabilities\n",
    "        # $$p(\\Delta x, \\Delta y) =\n",
    "        # \\sum_{j=1}^M \\Pi_j \\mathcal{N} \\big( \\Delta x, \\Delta y \\vert\n",
    "        # \\mu_{x,j}, \\mu_{y,j}, \\sigma_{x,j}, \\sigma_{y,j}, \\rho_{xy,j}\n",
    "        # \\big)$$\n",
    "        print(mix.log_prob(xy).shape, \"mix log prob shape\") # [seq_len, batch_size, n_distributions]\n",
    "        probs = torch.sum(pi.probs * torch.exp(mix.log_prob(xy)), 2)\n",
    "\n",
    "        # $$L_s = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_s} \\log \\big (p(\\Delta x, \\Delta y) \\big)$$\n",
    "        # Although `probs` has $N_{max}$ (`longest_seq_len`) elements, the sum is only taken\n",
    "        # upto $N_s$ because the rest is masked out.\n",
    "        #\n",
    "        # It might feel like we should be taking the sum and dividing by $N_s$ and not $N_{max}$,\n",
    "        # but this will give higher weight for individual predictions in shorter sequences.\n",
    "        # We give equal weight to each prediction $p(\\Delta x, \\Delta y)$ when we divide by $N_{max}$\n",
    "        print(probs.shape, \"probs shape\") # [seq_len, batch_size]\n",
    "        print(mask.shape, \"mask shape\") # [seq_len, batch_size]\n",
    "        loss_stroke = -torch.mean(mask * torch.log(1e-5 + probs))\n",
    "\n",
    "        # $$L_p = - \\frac{1}{N_{max}} \\sum_{i=1}^{N_{max}} \\sum_{k=1}^{3} p_{k,i} \\log(q_{k,i})$$\n",
    "        \n",
    "        print(target.shape, \" target shape\")\n",
    "        print(q_logits.shape, \" q_logits shape\")\n",
    "        loss_pen = -torch.mean(target[:, :, 2:] * q_logits)\n",
    "\n",
    "        # $$L_R = L_s + L_p$$\n",
    "        return loss_stroke + loss_pen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivLoss(Module):\n",
    "    \"\"\"\n",
    "    ## KL-Divergence loss\n",
    "\n",
    "    This calculates the KL divergence between a given normal distribution and $\\mathcal{N}(0, 1)$\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, sigma_hat: torch.Tensor, mu: torch.Tensor):\n",
    "        # $$L_{KL} = - \\frac{1}{2 N_z} \\bigg( 1 + \\hat{\\sigma} - \\mu^2 - \\exp(\\hat{\\sigma}) \\bigg)$$\n",
    "        return -0.5 * torch.mean(1 + sigma_hat - mu ** 2 - torch.exp(sigma_hat))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Sampler:\n",
    "    \"\"\"\n",
    "    ## Sampler\n",
    "\n",
    "    This samples a sketch from the decoder and plots it\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN):\n",
    "        self.decoder = decoder\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def sample(self, data: torch.Tensor, temperature: float):\n",
    "        # $N_{max}$\n",
    "        longest_seq_len = len(data)\n",
    "        \n",
    "        # Get $z$ from the encoder\n",
    "        z, _, _ = self.encoder(data)\n",
    "\n",
    "        print(z.shape, 'z shape')\n",
    "        # Start-of-sequence stroke is $(0, 0, 1, 0, 0)$\n",
    "        s = data.new_tensor([0, 0, 1, 0, 0])\n",
    "        seq = [s]\n",
    "        # Initial decoder is `None`.\n",
    "        # The decoder will initialize it to $[h_0; c_0] = \\tanh(W_{z}z + b_z)$\n",
    "        state = None\n",
    "\n",
    "        # We don't need gradients\n",
    "        with torch.no_grad():\n",
    "            # Sample $N_{max}$ strokes\n",
    "            for i in range(longest_seq_len):\n",
    "                # $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$ is the input to the decoder\n",
    "                print(\"original z shape\", z.shape)\n",
    "                print(s.view(1, 1, -1).shape, z.unsqueeze(0).shape, 'shapes')\n",
    "                data = torch.cat([s.view(1, 1, -1), z.unsqueeze(0)], 2)\n",
    "                print(data.shape, 'data shape')\n",
    "                # Get $\\Pi$, $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$,\n",
    "                # $q$ and the next state from the decoder\n",
    "                dist, q_logits, state = self.decoder(data, z, state)\n",
    "                # Sample a stroke\n",
    "                s = self._sample_step(dist, q_logits, temperature)\n",
    "                # Add the new stroke to the sequence of strokes\n",
    "                seq.append(s)\n",
    "                # Stop sampling if $p_3 = 1$. This indicates that sketching has stopped\n",
    "                if s[4] == 1:\n",
    "                    break\n",
    "\n",
    "        # Create a PyTorch tensor of the sequence of strokes\n",
    "        seq = torch.stack(seq)\n",
    "\n",
    "        # Plot the sequence of strokes\n",
    "        self.plot(seq)\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample_step(dist: 'BivariateGaussianMixture', q_logits: torch.Tensor, temperature: float):\n",
    "        # Set temperature $\\tau$ for sampling. This is implemented in class `BivariateGaussianMixture`.\n",
    "        dist.set_temperature(temperature)\n",
    "        # Get temperature adjusted $\\Pi$ and $\\mathcal{N}(\\mu_{x}, \\mu_{y}, \\sigma_{x}, \\sigma_{y}, \\rho_{xy})$\n",
    "        pi, mix = dist.get_distribution()\n",
    "        # Sample from $\\Pi$ the index of the distribution to use from the mixture\n",
    "        idx = pi.sample()[0, 0]\n",
    "\n",
    "        # Create categorical distribution $q$ with log-probabilities `q_logits` or $\\hat{q}$\n",
    "        q = torch.distributions.Categorical(logits=q_logits / temperature)\n",
    "        # Sample from $q$\n",
    "        q_idx = q.sample()[0, 0]\n",
    "\n",
    "        # Sample from the normal distributions in the mixture and pick the one indexed by `idx`\n",
    "        xy = mix.sample()[0, 0, idx]\n",
    "\n",
    "        # Create an empty stroke $(\\Delta x, \\Delta y, q_1, q_2, q_3)$\n",
    "        stroke = q_logits.new_zeros(5)\n",
    "        # Set $\\Delta x, \\Delta y$\n",
    "        stroke[:2] = xy\n",
    "        # Set $q_1, q_2, q_3$\n",
    "        stroke[q_idx + 2] = 1\n",
    "        #\n",
    "        return stroke\n",
    "\n",
    "    @staticmethod\n",
    "    def plot(seq: torch.Tensor):\n",
    "        # Take the cumulative sums of $(\\Delta x, \\Delta y)$ to get $(x, y)$\n",
    "        seq[:, 0:2] = torch.cumsum(seq[:, 0:2], dim=0)\n",
    "        # Create a new numpy array of the form $(x, y, q_2)$\n",
    "        seq[:, 2] = seq[:, 3]\n",
    "        seq = seq[:, 0:3].detach().cpu().numpy()\n",
    "\n",
    "        # Split the array at points where $q_2$ is $1$.\n",
    "        # i.e. split the array of strokes at the points where the pen is lifted from the paper.\n",
    "        # This gives a list of sequence of strokes.\n",
    "        strokes = np.split(seq, np.where(seq[:, 2] > 0)[0] + 1)\n",
    "        # Plot each sequence of strokes\n",
    "        for s in strokes:\n",
    "            plt.plot(s[:, 0], -s[:, 1])\n",
    "        # Don't show axes\n",
    "        plt.axis('off')\n",
    "        # Show the plot\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([118, 5]) original data shape\n",
      "torch.Size([118, 1, 5]) data shape\n",
      "torch.Size([1, 128]) z shape\n",
      "original z shape torch.Size([1, 128])\n",
      "torch.Size([1, 1, 5]) torch.Size([1, 1, 128]) shapes\n",
      "torch.Size([1, 1, 133]) data shape\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dataset = np.load(str('../data/sketchrnn/sketchrnn_bicycle.npz'), encoding='latin1', allow_pickle=True)\n",
    "encoder = EncoderRNN(128, 256)\n",
    "decoder = DecoderRNN(128, 256, 20)\n",
    "sampler = Sampler(encoder, decoder)\n",
    "\n",
    "# test sampler\n",
    "# bicyle dataset\n",
    "\n",
    "dataset = StrokesDataset(dataset['train'], 116)\n",
    "data, *_ = dataset[np.random.choice(len(dataset))]\n",
    "print(data.shape, 'original data shape')\n",
    "data = data.unsqueeze(1)\n",
    "print(data.shape, 'data shape')\n",
    "\n",
    "sampler.sample(data, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\"><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span><span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\">LABML WARNING</span></strong></span>\n",
       "<span style=\"color: #DDB62B\"><strong><span style=\"text-decoration: underline\"></span></strong></span><strong>.labml.yaml</strong> config file could not be found. Looking in path: <span style=\"color: #208FFB\">/Users/hunkim/Github/pixel-art-ai/sketchrnn</span><span style=\"color: #C5C1B4\"></span>\n",
       "<span style=\"color: #C5C1B4\">--------------------------------------------------</span></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 5]) torch.Size([1, 1, 128]) shapes\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n",
      "torch.Size([117, 4, 20]) mix log prob shape\n",
      "torch.Size([117, 4]) probs shape\n",
      "torch.Size([117, 4]) mask shape\n",
      "torch.Size([117, 4, 5])  target shape\n",
      "torch.Size([117, 4, 3])  q_logits shape\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 177\u001b[0m\n\u001b[1;32m    173\u001b[0m         configs\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 173\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m experiment\u001b[38;5;241m.\u001b[39mconfigs(configs, {\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer.optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# We use a learning rate of `1e-3` because we can see results faster.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m    169\u001b[0m })\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m experiment\u001b[38;5;241m.\u001b[39mstart():\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# Run the experiment\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/train_valid.py:269\u001b[0m, in \u001b[0;36mTrainValidConfigs.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_loop:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/train_valid.py:257\u001b[0m, in \u001b[0;36mTrainValidConfigs.run_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mupdate(is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39mnamespace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 257\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalidator:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracker\u001b[38;5;241m.\u001b[39mnamespace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/train_valid.py:140\u001b[0m, in \u001b[0;36mTrainer.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m         sm\u001b[38;5;241m.\u001b[39mon_epoch_start()\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\u001b[38;5;241m.\u001b[39mis_train):\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__iterate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_index\u001b[38;5;241m.\u001b[39mcompleted:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sm \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_modules:\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/train_valid.py:153\u001b[0m, in \u001b[0;36mTrainer.__iterate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_index\u001b[38;5;241m.\u001b[39miteration_completed:\n\u001b[1;32m    151\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__iterable)\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_index\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    156\u001b[0m     monit\u001b[38;5;241m.\u001b[39mprogress(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_index\u001b[38;5;241m.\u001b[39mepoch_progress)\n",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m, in \u001b[0;36mConfigs.step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Encode the sequence of strokes\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m monit\u001b[38;5;241m.\u001b[39msection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# Get $z$, $\\mu$, and $\\hat{\\sigma}$\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m     z, mu, sigma_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Decode the mixture of distributions and $\\hat{q}$\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m monit\u001b[38;5;241m.\u001b[39msection(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# Concatenate $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m, in \u001b[0;36mEncoderRNN.forward\u001b[0;34m(self, inputs, state)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: torch\u001b[38;5;241m.\u001b[39mTensor, state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# The hidden state of the bidirectional LSTM is the concatenation of the\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# output of the last token in the forward direction and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# h_{\\leftarrow} = encodeâ†_{\\leftarrow}(S_{reverse}),\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# h = [h_{\\rightarrow}; h_{\\leftarrow}]$$\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     _, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# The state has shape `[2, batch_size, hidden_size]`,\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# where the first dimension is the direction.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# We rearrange it to get $h = [h_{\\rightarrow}; h_{\\leftarrow}]$\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# hidden = einops.rearrange(hidden, 'fb b h -> b (fb h)')\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     hidden \u001b[38;5;241m=\u001b[39m hidden\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/torch/nn/modules/rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "File \u001b[0;32m~/Github/pixel-art-ai/.venv/lib/python3.11/site-packages/labml_helpers/training_loop.py:175\u001b[0m, in \u001b[0;36mTrainingLoop.__handler\u001b[0;34m(self, sig, frame)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__finish()\n\u001b[1;32m    174\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKilling loop...\u001b[39m\u001b[38;5;124m'\u001b[39m, Text\u001b[38;5;241m.\u001b[39mdanger)\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mold_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class Configs(TrainValidConfigs):\n",
    "    \"\"\"\n",
    "    ## Configurations\n",
    "\n",
    "    These are default configurations which can later be adjusted by passing a `dict`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Device configurations to pick the device to run the experiment\n",
    "    device: torch.device = DeviceConfigs()\n",
    "    #\n",
    "    encoder: EncoderRNN\n",
    "    decoder: DecoderRNN\n",
    "    optimizer: optim.Adam\n",
    "    sampler: Sampler\n",
    "\n",
    "    dataset_name: str\n",
    "    train_loader: DataLoader\n",
    "    valid_loader: DataLoader\n",
    "    train_dataset: StrokesDataset\n",
    "    valid_dataset: StrokesDataset\n",
    "\n",
    "    # Encoder and decoder sizes\n",
    "    enc_hidden_size = 256\n",
    "    dec_hidden_size = 512\n",
    "\n",
    "    # Batch size\n",
    "    batch_size = 4\n",
    "\n",
    "    # Number of features in $z$\n",
    "    d_z = 128\n",
    "    # Number of distributions in the mixture, $M$\n",
    "    n_distributions = 20\n",
    "\n",
    "    # Weight of KL divergence loss, $w_{KL}$\n",
    "    kl_div_loss_weight = 0.5\n",
    "    # Gradient clipping\n",
    "    grad_clip = 1.\n",
    "    # Temperature $\\tau$ for sampling\n",
    "    temperature = 0.4\n",
    "\n",
    "    # Filter out stroke sequences longer than $200$\n",
    "    max_seq_length = 200\n",
    "\n",
    "    epochs = 1 \n",
    "\n",
    "    kl_div_loss = KLDivLoss()\n",
    "    reconstruction_loss = ReconstructionLoss()\n",
    "\n",
    "    def init(self):\n",
    "        # Initialize encoder & decoder\n",
    "        self.encoder = EncoderRNN(self.d_z, self.enc_hidden_size).to(self.device)\n",
    "        self.decoder = DecoderRNN(self.d_z, self.dec_hidden_size, self.n_distributions).to(self.device)\n",
    "\n",
    "        # Set optimizer. Things like type of optimizer and learning rate are configurable\n",
    "        optimizer = OptimizerConfigs()\n",
    "        optimizer.parameters = list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        # Create sampler\n",
    "        self.sampler = Sampler(self.encoder, self.decoder)\n",
    "\n",
    "        # `npz` file path is `data/sketch/[DATASET NAME].npz`\n",
    "        path = lab.get_data_path() / 'sketch' / f'{self.dataset_name}.npz'\n",
    "        # Load the numpy file\n",
    "        dataset = np.load(str(path), encoding='latin1', allow_pickle=True)\n",
    "\n",
    "        # Create training dataset\n",
    "        self.train_dataset = StrokesDataset(dataset['train'], self.max_seq_length)\n",
    "        # Create validation dataset\n",
    "        self.valid_dataset = StrokesDataset(dataset['valid'], self.max_seq_length, self.train_dataset.scale)\n",
    "\n",
    "        # Create training data loader\n",
    "        self.train_loader = DataLoader(self.train_dataset, self.batch_size, shuffle=True)\n",
    "        # Create validation data loader\n",
    "        self.valid_loader = DataLoader(self.valid_dataset, self.batch_size)\n",
    "\n",
    "        # Add hooks to monitor layer outputs on Tensorboard\n",
    "        hook_model_outputs(self.mode, self.encoder, 'encoder')\n",
    "        hook_model_outputs(self.mode, self.decoder, 'decoder')\n",
    "\n",
    "        # Configure the tracker to print the total train/validation loss\n",
    "        tracker.set_scalar(\"loss.total.*\", True)\n",
    "\n",
    "        self.state_modules = []\n",
    "\n",
    "    def step(self, batch: Any, batch_idx: BatchIndex):\n",
    "        self.encoder.train(self.mode.is_train)\n",
    "        self.decoder.train(self.mode.is_train)\n",
    "\n",
    "        # Move `data` and `mask` to device and swap the sequence and batch dimensions.\n",
    "        # `data` will have shape `[seq_len, batch_size, 5]` and\n",
    "        # `mask` will have shape `[seq_len, batch_size]`.\n",
    "        data = batch[0].to(self.device).transpose(0, 1)\n",
    "        mask = batch[1].to(self.device).transpose(0, 1)\n",
    "\n",
    "        # Increment step in training mode\n",
    "        if self.mode.is_train:\n",
    "            tracker.add_global_step(len(data))\n",
    "\n",
    "        # Encode the sequence of strokes\n",
    "        with monit.section(\"encoder\"):\n",
    "            # Get $z$, $\\mu$, and $\\hat{\\sigma}$\n",
    "            z, mu, sigma_hat = self.encoder(data)\n",
    "\n",
    "        # Decode the mixture of distributions and $\\hat{q}$\n",
    "        with monit.section(\"decoder\"):\n",
    "            # Concatenate $[(\\Delta x, \\Delta y, p_1, p_2, p_3); z]$\n",
    "            z_stack = z.unsqueeze(0).expand(data.shape[0] - 1, -1, -1)\n",
    "            inputs = torch.cat([data[:-1], z_stack], 2)\n",
    "            # Get mixture of distributions and $\\hat{q}$\n",
    "            dist, q_logits, _ = self.decoder(inputs, z, None)\n",
    "\n",
    "        # Compute the loss\n",
    "        with monit.section('loss'):\n",
    "            # $L_{KL}$\n",
    "            kl_loss = self.kl_div_loss(sigma_hat, mu)\n",
    "            # $L_R$\n",
    "            reconstruction_loss = self.reconstruction_loss(mask, data[1:], dist, q_logits)\n",
    "            # $Loss = L_R + w_{KL} L_{KL}$\n",
    "            loss = reconstruction_loss + self.kl_div_loss_weight * kl_loss\n",
    "\n",
    "            # Track losses\n",
    "            tracker.add(\"loss.kl.\", kl_loss)\n",
    "            tracker.add(\"loss.reconstruction.\", reconstruction_loss)\n",
    "            tracker.add(\"loss.total.\", loss)\n",
    "\n",
    "        # Only if we are in training state\n",
    "        if self.mode.is_train:\n",
    "            # Run optimizer\n",
    "            with monit.section('optimize'):\n",
    "                # Set `grad` to zero\n",
    "                self.optimizer.zero_grad()\n",
    "                # Compute gradients\n",
    "                loss.backward()\n",
    "                # Log model parameters and gradients\n",
    "                if batch_idx.is_last:\n",
    "                    tracker.add(encoder=self.encoder, decoder=self.decoder)\n",
    "                # Clip gradients\n",
    "                nn.utils.clip_grad_norm_(self.encoder.parameters(), self.grad_clip)\n",
    "                nn.utils.clip_grad_norm_(self.decoder.parameters(), self.grad_clip)\n",
    "                # Optimize\n",
    "                self.optimizer.step()\n",
    "\n",
    "        tracker.save()\n",
    "\n",
    "    def sample(self):\n",
    "        # Randomly pick a sample from validation dataset to encoder\n",
    "        data, *_ = self.valid_dataset[np.random.choice(len(self.valid_dataset))]\n",
    "        # Add batch dimension and move it to device\n",
    "        data = data.unsqueeze(1).to(self.device)\n",
    "        # Sample\n",
    "        self.sampler.sample(data, self.temperature)\n",
    "\n",
    "\n",
    "def main():\n",
    "    configs = Configs()\n",
    "    experiment.create(name=\"sketch_rnn\")\n",
    "\n",
    "    # Pass a dictionary of configurations\n",
    "    experiment.configs(configs, {\n",
    "        'optimizer.optimizer': 'Adam',\n",
    "        # We use a learning rate of `1e-3` because we can see results faster.\n",
    "        # Paper had suggested `1e-4`.\n",
    "        'optimizer.learning_rate': 1e-3,\n",
    "        # Name of the dataset\n",
    "        'dataset_name': 'bicycle',\n",
    "        # Number of inner iterations within an epoch to switch between training, validation and sampling.\n",
    "        'inner_iterations': 10\n",
    "    })\n",
    "\n",
    "    with experiment.start():\n",
    "        # Run the experiment\n",
    "        configs.run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample \n",
    "# !python3 sketch_rnn.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
