{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NpzFile '../data/sketchrnn/sketchrnn_airplane.npz' with keys: test, train, valid\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = \"../data/sketchrnn/\"\n",
    "dataset_name = \"sketchrnn_airplane.npz\"\n",
    "dict_data = np.load(dataset_dir + dataset_name, encoding='latin1', allow_pickle=True)\n",
    "# extract the first array\n",
    "\n",
    "print(dict_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "\n",
    "train = dict_data['train']\n",
    "print(train.shape) # we have 70000 sketches\n",
    "# each seq length varies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 3) (56, 3) (39, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train[0].shape, train[1].shape, train[2].shape) # each sketch has a different length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-52,  -4,   0], dtype=int16)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 3) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      5\u001b[0m b \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(b)\n\u001b[1;32m      7\u001b[0m c \u001b[38;5;241m=\u001b[39m [a, b]\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (2, 3) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "a = [[1,2,3,4], [5,6,7,8], [9,10,11,12]]\n",
    "b = [[\"a\", \"b\", \"c\"], [\"d\", \"e\", \"f\"], [\"g\", \"h\", \"i\"]]\n",
    "\n",
    "a = np.array(a)\n",
    "b = np.array(b)\n",
    "\n",
    "c = [a, b]\n",
    "\n",
    "\n",
    "print(np.array(c).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero\n",
    "from dezero import cuda\n",
    "from typing import List, Optional, Tuple, Any\n",
    "import math\n",
    "\n",
    "class StrokesDataset(dezero.DataLoader):\n",
    "    def __init__(self, data, batch_size, max_seq_length: int, scale: Optional[float] = None, shuffle=True, gpu=False):\n",
    "        stroke_data = []\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.data_size = len(data)\n",
    "        self.max_iter = math.ceil(self.data_size / batch_size)\n",
    "        self.gpu = gpu\n",
    "        \n",
    "        xp = cuda.cupy if self.gpu else np\n",
    "        \n",
    "        for seq in data:\n",
    "            # we will deem a sequence that is less than 10 as too short and thus ignore it\n",
    "            if 10 < len(seq) <= max_seq_length:\n",
    "                # clamp the delta x and delta y to [-1000, 1000]\n",
    "                seq = np.minimum(seq, 1000)\n",
    "                seq = np.maximum(seq, -1000)\n",
    "                \n",
    "                seq = np.array(seq, dtype=np.float32)\n",
    "                stroke_data.append(seq)\n",
    "        \n",
    "        if scale is None:\n",
    "            # calculate the scale factor\n",
    "            # the scale factor is the standard deviation of the x and y coordinates\n",
    "            # mean is not adjusted for simplicity\n",
    "            # 0:2 means the first two columns of the array which are x and y coordinates\n",
    "            scale = np.std(np.concatenate([np.ravel(s[:,0:2]) for s in stroke_data]))\n",
    "        \n",
    "        longest_seq_len = max([len(seq) for seq in stroke_data])\n",
    "        \n",
    "        # we add two extra columns to the dataset since we currently there are only 3 columns in the dataset\n",
    "        # additional two columns are for changing the last point 1/0 to a one-hot vector\n",
    "        temp_stroke_dataset = xp.zeros((len(stroke_data), longest_seq_len + 2, 5), dtype=np.float32)\n",
    "        \n",
    "        # self.mask is used to mark areas of the sequence that are not used\n",
    "        # we first initialize it to zero\n",
    "        temp_mask_dataset = xp.zeros((len(stroke_data), longest_seq_len + 1))\n",
    "        \n",
    "        self.dataset = []\n",
    "        \n",
    "        # start of sequence is [0, 0, 1, 0, 0]\n",
    "        \n",
    "        for i, seq in enumerate(stroke_data):\n",
    "            seq = xp.array(seq, dtype=xp.float32)\n",
    "            len_seq = len(seq)\n",
    "            \n",
    "            # we start from 1 to leave the first row for the start of sequence token\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 0:2] = seq[:, :2] / scale # this is the x and y coordinates\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 2] = 1 - seq[:, 2] # this is the pen down\n",
    "            temp_stroke_dataset[i, 1:len_seq + 1, 3] = seq[:, 2] # this is the pen up\n",
    "            temp_stroke_dataset[i, len_seq + 1, 4] = 1  # this is the end of sequence token\n",
    "            temp_mask_dataset[i, :len_seq + 1] = 1 # mask is on until the end of the sequence \n",
    "            # self.mask is used to mark areas of the sequence that are not used\n",
    "            # for example, if the sequence is shorter than the longest sequence, we use mask to ignore the rest of the sequence\n",
    "            # an example of mask is [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        \n",
    "        temp_stroke_dataset[:, 0, 2] = 1\n",
    "        \n",
    "        for i in range(len(stroke_data)):\n",
    "            self.dataset.append([temp_stroke_dataset[i], temp_mask_dataset[i]])\n",
    "        \n",
    "        \n",
    "        self.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "strokes = StrokesDataset(train, batch_size=4, max_seq_length=200, gpu=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000\n",
      "(4, 101, 5) (4, 100)\n",
      "34 34\n",
      "57 57\n",
      "40 40\n",
      "86 86\n"
     ]
    }
   ],
   "source": [
    "print(strokes.data_size)\n",
    "# first item\n",
    "\n",
    "\n",
    "\n",
    "x, t = strokes.__next__()\n",
    "print(x.shape, t.shape) # x is the stroke, t is the mask (x has one more column than t)\n",
    "\n",
    "\n",
    "# check if the mask is working\n",
    "batch_size = x.shape[0]\n",
    "\n",
    "for i in range(batch_size):\n",
    "    mask_zero_id = np.where(t[i] == 0)[0]\n",
    "    # first id\n",
    "    first_id = mask_zero_id[0]\n",
    "    stroke_end_id = np.where(x[i, :, 4] == 1)[0]\n",
    "    first_stroke_end_id = stroke_end_id[0]\n",
    "    \n",
    "    print(first_id, first_stroke_end_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero.functions as F\n",
    "\n",
    "# According to other estimates\n",
    "# the number of distributions in the mixture model is 20\n",
    "# https://github.com/Shun14/sketch-rnn-kanji\n",
    "# https://nn.labml.ai/sketch_rnn/index.html\n",
    "\n",
    "# This is for getting the loss of delta_x and delta_y\n",
    "class BivariateGaussianMixture:\n",
    "    def __init__(self, pi_logits, mu_x, mu_y, sigma_x, sigma_y, rho_xy):\n",
    "        self.pi_logits = pi_logits\n",
    "        self.mu_x = mu_x\n",
    "        self.mu_y = mu_y\n",
    "        self.sigma_x = sigma_x\n",
    "        self.sigma_y = sigma_y\n",
    "        self.rho_xy = rho_xy\n",
    "    \n",
    "    @property\n",
    "    def n_distributions(self):\n",
    "        return self.pi_logits.shape[-1]\n",
    "    \n",
    "    def set_temperature(self, temperature: float):\n",
    "        self.pi_logits /= temperature\n",
    "        self.sigma_x *= math.sqrt(temperature)\n",
    "        self.sigma_y *= math.sqrt(temperature)\n",
    "    \n",
    "    def gaussian_pdf(self, x_delta, y_delta):\n",
    "        # the result means the probability of y in the normal distribution\n",
    "        # we check the probability of y in the normal distribution\n",
    "        # if the probability is high, the result is close to 1\n",
    "        norm1 = F.sub(x_delta, self.mu_x)\n",
    "        norm2 = F.sub(y_delta, self.mu_y)\n",
    "        xp = cuda.get_array_module(norm1)\n",
    "\n",
    "        \n",
    "        s1s2 = F.mul(self.sigma_x, self.sigma_y)\n",
    "        \n",
    "        # This is from: https://github.com/hardmaru/write-rnn-tensorflow/blob/master/model.py\n",
    "        # z = tf.square(tf.div(norm1, s1)) + tf.square(tf.div(norm2, s2))\n",
    "        #     - 2 * tf.div(tf.multiply(rho, tf.multiply(norm1, norm2)), s1s2)\n",
    "         \n",
    "        # below is the deconstruction of the above linez\n",
    "        z_first_term = F.pow(F.div(norm1, self.sigma_x), 2)\n",
    "        z_second_term = F.pow(F.div(norm2, self.sigma_y), 2)\n",
    "        z_last_term_inner = F.mul(self.rho_xy, F.mul(norm1, norm2))\n",
    "        z_last_term_middle = F.div(z_last_term_inner, s1s2)\n",
    "        tmp_z = np.ones(z_last_term_middle.shape) * -2\n",
    "        z_last_term = F.mul(tmp_z, z_last_term_middle)\n",
    "        z = F.add(F.add(z_first_term, z_second_term), z_last_term)\n",
    "        negRho = F.sub(np.ones(self.rho_xy.shape), F.pow(self.rho_xy, 2))\n",
    "\n",
    "        \n",
    "        result = F.exp(F.div(-z, 2 * negRho))\n",
    "        deno_first_term = np.ones(self.sigma_x.shape) * 2 * math.pi\n",
    "        denom_second_term = F.mul(s1s2, F.pow(negRho, 0.5))\n",
    "        denom = F.mul(deno_first_term, denom_second_term)\n",
    "        result = F.div(result, denom)\n",
    "        \n",
    "        return result\n",
    "\n",
    "    # x1_data and x2_data are the real x and y coordinates of the stroke\n",
    "    def get_lossfunc(self, x_delta, y_delta):\n",
    "        result0 = self.gaussian_pdf(x_delta, y_delta)\n",
    "        \n",
    "        elipson = 1e-20\n",
    "        # check if result0 has inf or nan\n",
    "        result1 = F.mul(result0, self.pi_logits)\n",
    "        result1 = F.sum(result1, axis=1, keepdims=True)\n",
    "        result1 = -F.log(result1)\n",
    "        \n",
    "        return F.mean(result1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dezero.models as M\n",
    "import dezero.layers as L\n",
    "\n",
    "class EncoderRNN(M.Model):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lstm = L.LSTM(5, n_hidden)\n",
    "        self.mu_head = L.Linear(2 * n_hidden, n_output)\n",
    "        self.sigma_head = L.Linear(2 * n_hidden, n_output)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.rnn.reset_state()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.lstm(x)\n",
    "        \n",
    "        mu = self.mu_head(h)\n",
    "        \n",
    "        sigma_hat = self.sigma_head(h)\n",
    "        \n",
    "        sigma = F.exp(F.div(sigma_hat, 2))\n",
    "        \n",
    "        xp = cuda.get_array_module(mu)\n",
    "        \n",
    "        z = mu + sigma * xp.random.normal(0, 1, mu.shape) # reparameterization trick\n",
    "        \n",
    "        return z, mu, sigma\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'M' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDecoderRNN\u001b[39;00m(\u001b[43mM\u001b[49m\u001b[38;5;241m.\u001b[39mModel):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, d_z: \u001b[38;5;28mint\u001b[39m, n_hidden: \u001b[38;5;28mint\u001b[39m, n_distributions: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'M' is not defined"
     ]
    }
   ],
   "source": [
    "class DecoderRNN(M.Model):\n",
    "    def __init__(self, d_z: int, n_hidden: int, n_distributions: int):\n",
    "        super().__init__()\n",
    "        self.lstm = L.LSTM(d_z + 5, n_hidden)\n",
    "        self.init_state = L.Linear(d_z, n_hidden) # initial state of lstm is [h0;c0] = tanh(Wz * z + bz)\n",
    "        self.mixtures = L.Linear(n_hidden, 6 * n_distributions)\n",
    "        self.q_head = L.Linear(n_hidden, 3) # this is for logit q1 and q2 and q3\n",
    "        self.q_log_softmax = F.LogSoftmax(-1) \n",
    "        \n",
    "        self.n_distributions = n_distributions\n",
    "        self.n_hidden = n_hidden\n",
    "    \n",
    "    def forward(self, x, z, state = None):\n",
    "        xp = cuda.get_array_module(x)\n",
    "        if state is None:\n",
    "            temp = F.tanh(self.init_state(z))\n",
    "            # split the state into h and c into n_hidden groups\n",
    "            h, c = xp.split(temp, self.n_hidden, axis=1)\n",
    "            # make h and c into 1d array\n",
    "            state = (h.rave(), c.ravel())\n",
    "        \n",
    "        outputs, state = self.lstm(x, state)\n",
    "        \n",
    "        q_logits = self.q_log_softmax(self.q_head(outputs))\n",
    "        \n",
    "        pi_logts, mu_x, mu_y, sigma_x, sigma_y, rho_xy = xp.split(self.mixtures(outputs), self.n_distributions, axis=2)\n",
    "        \n",
    "        BGM = BivariateGaussianMixture(pi_logts, mu_x, mu_y, sigma_x, sigma_y, rho_xy)\n",
    "        \n",
    "        return q_logits, BGM, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReconstructionLoss(M.Model):\n",
    "    def forward(self, mask, target, bgm, q_logits):\n",
    "        xp = cuda.get_array_module(mask)\n",
    "        # target is a 3 dimensional array\n",
    "        # xy = target[:, :, 0:2].unsqueeze(-2).expand(-1, -1, dist.n_distributions, -1)\n",
    "        xy = target[:, :, 0:2]\n",
    "        xy = xy[:, :, xp.newaxis, :]\n",
    "        \n",
    "        expanded_shape = (xy.shape[0], xy.shape[1], bgm.n_distributions, xy.shape[3])\n",
    "        \n",
    "        x = xp.tile(xy, expanded_shape)\n",
    "        y = xp.tile(xy, expanded_shape)\n",
    "        loss_stroke = bgm.get_lossfunc(x, y)\n",
    "        \n",
    "        loss_pen = -F.mean(F.mul(target[:,:,2:], q_logits))\n",
    "        \n",
    "        return F.add(loss_stroke, loss_pen)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KLDivergenceLoss(M.Model):\n",
    "    def forward(self, mu, sigma):\n",
    "        xp = cuda.get_array_module(mu)\n",
    "        tmp = xp.ones(sigma.shape)\n",
    "        inner_1 = F.add(tmp, sigma)\n",
    "        inner_2 = F.add(F.pow(mu, 2), F.exp(sigma))\n",
    "        inner = F.sub(inner_1, inner_2)\n",
    "        tmp2 = xp.ones(inner.shape) * -2\n",
    "        return F.mean(F.div(inner, tmp2))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
